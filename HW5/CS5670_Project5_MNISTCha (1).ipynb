{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJ1zvCUxhz_Y",
    "colab_type": "text"
   },
   "source": [
    "# CS5670 Project 5 - MNIST Challenge Notebook\n",
    "\n",
    "Welcome to the Project 5 4-credit MNIST Challenge.\n",
    "\n",
    "In this notebook you will create and train your own neural network model in PyTorch for classifying handwritten digits on the MNIST dataset.\n",
    "We will pretend that we are targeting low-power and low-memory devices, and so you will have to limit the number of model parameters that you use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OvSJugE7bql",
    "colab_type": "text"
   },
   "source": [
    "## Installation of Dependencies\n",
    "\n",
    "First you want to **make sure that you're using the GPU backend**.\n",
    "\n",
    "* Go to Runtime -> Change runtime type and choose \"GPU\" under Hardware Accelerator.\n",
    "* Only then run the next cell. It should print \"GPU Support: True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "x4Wlyq6F7Vny",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "a7cbd619-f18f-44cb-e9e2-97d0199e753f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.525468740079E12,
     "user_tz": 240.0,
     "elapsed": 2410.0,
     "user": {
      "displayName": "Disheng Zheng",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "111016125128010800049"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Support: True\n"
     ]
    }
   ],
   "source": [
    "# http://pytorch.org/\n",
    "from os import path\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "\n",
    "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
    "import torch\n",
    "print(f\"GPU Support: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHLKy2Ekqoq4",
    "colab_type": "text"
   },
   "source": [
    "## Useful functions\n",
    "\n",
    "Run the below cell to define functions that will be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "XPWrOExkqysJ",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import urllib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os, sys,  math, random, subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "from google.protobuf import text_format\n",
    "from io import StringIO\n",
    "import PIL.Image\n",
    "\n",
    "def get_n_params(module):\n",
    "  nparam = 0\n",
    "  for name, param in module.named_parameters():\n",
    "    param_count = 1\n",
    "    for size in list(param.size()):\n",
    "      param_count *= size\n",
    "    nparam += param_count\n",
    "  return nparam\n",
    "\n",
    "def get_model_params(model):\n",
    "  nparam = 0\n",
    "  for name, module in model.named_modules():\n",
    "    nparam += get_n_params(module)\n",
    "  return nparam\n",
    "\n",
    "def to_numpy_image(tensor_or_variable):\n",
    "  \n",
    "  # If this is already a numpy image, just return it\n",
    "  if type(tensor_or_variable) == np.ndarray:\n",
    "    return tensor_or_variable\n",
    "  \n",
    "  # Make sure this is a tensor and not a variable\n",
    "  if type(tensor_or_variable) == Variable:\n",
    "    tensor = tensor_or_variable.data\n",
    "  else:\n",
    "    tensor = tensor_or_variable\n",
    "  \n",
    "  # Convert to numpy and move to CPU if necessary\n",
    "  np_img = tensor.cpu().numpy()\n",
    "  \n",
    "  # If there is no batch dimension, add one\n",
    "  if len(np_img.shape) == 3:\n",
    "    np_img = np_img[np.newaxis, ...]\n",
    "  \n",
    "  # Convert from BxCxHxW (PyTorch convention) to BxHxWxC (OpenCV/numpy convention)\n",
    "  np_img = np_img.transpose(0, 2, 3, 1)\n",
    "  \n",
    "  return np_img\n",
    "\n",
    "def normalize_zero_one_range(tensor_like):\n",
    "  x = tensor_like - tensor_like.min()\n",
    "  x = x / (x.max() + 1e-9)\n",
    "  return x\n",
    "\n",
    "def prep_for_showing(image):\n",
    "  np_img = to_numpy_image(image)\n",
    "  if len(np_img.shape) > 3:\n",
    "    np_img = np_img[0]\n",
    "  np_img = normalize_zero_one_range(np_img)\n",
    "  return np_img\n",
    "\n",
    "def show_image(tensor_var_or_np, title=None, bordercolor=None):\n",
    "  np_img = prep_for_showing(tensor_var_or_np)\n",
    "  \n",
    "  if bordercolor is not None:\n",
    "    np_img = draw_border(np_img, bordercolor)\n",
    "  \n",
    "  # plot it\n",
    "  np_img = np_img.squeeze()\n",
    "  plt.figure(figsize=(4,4))\n",
    "  plt.imshow(np_img)\n",
    "  plt.axis('off')\n",
    "  if title: plt.title(title)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJiMTcx9qr5Z",
    "colab_type": "text"
   },
   "source": [
    "## Training Data\n",
    "\n",
    "We will use the [MNIST handrwritten digit dataset](http://yann.lecun.com/exdb/mnist/) to train our neural network models. There is a simple wrapper for the MNIST dataset in the torchvision package that implements the Dataset class. We will use that in conjunction with the DataLoader to load training data. Run the below cell to download and initialize our training and test datasets. You should see an example batch of images and their labels shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hURbcBfwqUrY",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 201.0
    },
    "outputId": "61e9ec6e-a768-4d59-a9c7-5f8fc3e45df4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.525468742881E12,
     "user_tz": 240.0,
     "elapsed": 702.0,
     "user": {
      "displayName": "Disheng Zheng",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "111016125128010800049"
     }
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAACTCAYAAACH3gx7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f33772749b0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:      7     2     1     0     4     1     4     9     5     9     0     6     9     0     1     5     9     7     3     4     9     6     6     5     4     0     7     4     0     1     3     1\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "show_image(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % labels[j] for j in range(BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfN6oCjwrqR1",
    "colab_type": "text"
   },
   "source": [
    "## Model Definition (Example Model)\n",
    "\n",
    "This is an example model provided to you. It is pretty good and it reaches >98% accuracy on the test set, but we can do better. In particular, we will try to break the 99% accuracy barrier with a tight limit on the number of model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jjvxIDrXrujd",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "91a8d39b-c9ae-4f5f-d194-8b4c0ea616b7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.525468866521E12,
     "user_tz": 240.0,
     "elapsed": 327.0,
     "user": {
      "displayName": "Disheng Zheng",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "111016125128010800049"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model number of parameters: 20842\n"
     ]
    }
   ],
   "source": [
    "class ExampleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExampleModel, self).__init__()\n",
    "        # Convolution. Input channels: 1, output channels: 6, kernel size: 5\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # Max-pooling layer that will halve the HxW resolution\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Another 5x5 convolution that brings channel count up to 16\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # Three fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 60)\n",
    "        self.fc2 = nn.Linear(60, 40)\n",
    "        self.fc3 = nn.Linear(40, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolution, activation and pooling\n",
    "        # Output width after convolution = (input_width - (kernel_size - 1) / 2)\n",
    "        # Output width after pooling = input_width / 2\n",
    "        \n",
    "        # x.size() = Bx1x28x28\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # x.size() = Bx6x12x12\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # x.size() = Bx16x4x4\n",
    "        \n",
    "        # Flatten the output\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = ExampleModel()\n",
    "\n",
    "nparams = get_n_params(net)\n",
    "print(f\"Model number of parameters: {nparams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujgr4bu-wdD4",
    "colab_type": "text"
   },
   "source": [
    "## Model Definition (Your Model)\n",
    "\n",
    "Define your own model here. It should satisfy the following constraints:\n",
    "<font color=\"red\">\n",
    "*** Number of total model parameters: < 50000**\n",
    "<br/>\n",
    "*** Reaches test-set accuracy greater than or equal to 99% after training for 10 epochs**\n",
    "</font>\n",
    "  \n",
    "A couple of ideas you can try:\n",
    "* Different model structure (e.g. more layers, smaller/bigger kernels)\n",
    "* Residual connections\n",
    "* Batch [2] / Layer Normalization [3]\n",
    "* Densely connected architectures [1]\n",
    "\n",
    "<font size=\"1em\">[1] Huang, G., Liu, Z., Weinberger, K. Q., & van der Maaten, L. (2017, July). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (Vol. 1, No. 2, p. 3).</font>\n",
    "</br>\n",
    "<font size=\"1em\">[2] Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167.</font>\n",
    "</br>\n",
    "<font size=\"1em\">[3] Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer normalization. arXiv preprint arXiv:1607.06450.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "k2CMdk8LKGdW",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "outputId": "035ed5a8-c67c-41fb-f992-b66f66ae6bde",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.525468870946E12,
     "user_tz": 240.0,
     "elapsed": 272.0,
     "user": {
      "displayName": "Disheng Zheng",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "111016125128010800049"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model number of parameters: 49600\n"
     ]
    }
   ],
   "source": [
    "class StudentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentModel, self).__init__()\n",
    "        # Convolution. Input channels: 1, output channels: 16, kernel size: 5\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
    "        # Max-pooling layer that will halve the HxW resolution\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Another 5x5 convolution that brings channel count up to 34\n",
    "        self.conv2 = nn.Conv2d(16, 34, 5)\n",
    "\n",
    "        # Three fully connected layers\n",
    "        self.fc1 = nn.Linear(34 * 4 * 4, 60)\n",
    "        self.fc2 = nn.Linear(60, 40)\n",
    "        self.fc3 = nn.Linear(40, 10)       \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolution, activation and pooling\n",
    "        # Output width after convolution = (input_width - (kernel_size - 1) / 2)\n",
    "        # Output width after pooling = input_width / 2\n",
    "        # x.size() = Bx1x28x28\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # x.size() = Bx16x12x12\n",
    "        #Dropout 0.2 of the points data to avoid overfit.\n",
    "        x = F.dropout(self.pool(F.relu(self.conv2(x))), 0.2)\n",
    "        # x.size() = Bx34x4x4\n",
    "        \n",
    "        # Flatten the output\n",
    "        x = x.view(-1, 34 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #Dropout 0.2 of the points data to avoid overfit.\n",
    "        x = F.dropout(F.relu(self.fc2(x)), 0.2)    \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "      \n",
    "net = StudentModel()\n",
    "\n",
    "nparams = get_n_params(net)\n",
    "print(f\"Model number of parameters: {nparams}\")\n",
    "\n",
    "assert nparams < 50000, \"Too many model parameters!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NBY2H8yrgRE",
    "colab_type": "text"
   },
   "source": [
    "## Training Loop\n",
    "\n",
    "Run the following training loop to train your model. **Uncomment the line `net = StudentModel()` to use your model instead of the example model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "EVzqqVy9rfLk",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2635.0
    },
    "outputId": "3638964c-a885-40ca-f234-636807d3adc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "[1,   100] loss: 2.085\n",
      "[1,   200] loss: 0.596\n",
      "[1,   300] loss: 0.255\n",
      "[1,   400] loss: 0.177\n",
      "[1,   500] loss: 0.184\n",
      "[1,   600] loss: 0.142\n",
      "[1,   700] loss: 0.132\n",
      "[1,   800] loss: 0.117\n",
      "[1,   900] loss: 0.091\n",
      "[1,  1000] loss: 0.094\n",
      "[1,  1100] loss: 0.101\n",
      "[1,  1200] loss: 0.111\n",
      "[1,  1300] loss: 0.089\n",
      "[1,  1400] loss: 0.088\n",
      "[1,  1500] loss: 0.093\n",
      "[1,  1600] loss: 0.077\n",
      "[1,  1700] loss: 0.059\n",
      "[1,  1800] loss: 0.078\n",
      "[2,   100] loss: 0.057\n",
      "[2,   200] loss: 0.047\n",
      "[2,   300] loss: 0.049\n",
      "[2,   400] loss: 0.062\n",
      "[2,   500] loss: 0.061\n",
      "[2,   600] loss: 0.047\n",
      "[2,   700] loss: 0.064\n",
      "[2,   800] loss: 0.055\n",
      "[2,   900] loss: 0.052\n",
      "[2,  1000] loss: 0.060\n",
      "[2,  1100] loss: 0.055\n",
      "[2,  1200] loss: 0.070\n",
      "[2,  1300] loss: 0.055\n",
      "[2,  1400] loss: 0.049\n",
      "[2,  1500] loss: 0.053\n",
      "[2,  1600] loss: 0.041\n",
      "[2,  1700] loss: 0.037\n",
      "[2,  1800] loss: 0.050\n",
      "[3,   100] loss: 0.031\n",
      "[3,   200] loss: 0.037\n",
      "[3,   300] loss: 0.046\n",
      "[3,   400] loss: 0.053\n",
      "[3,   500] loss: 0.043\n",
      "[3,   600] loss: 0.029\n",
      "[3,   700] loss: 0.035\n",
      "[3,   800] loss: 0.048\n",
      "[3,   900] loss: 0.035\n",
      "[3,  1000] loss: 0.042\n",
      "[3,  1100] loss: 0.034\n",
      "[3,  1200] loss: 0.043\n",
      "[3,  1300] loss: 0.044\n",
      "[3,  1400] loss: 0.040\n",
      "[3,  1500] loss: 0.032\n",
      "[3,  1600] loss: 0.036\n",
      "[3,  1700] loss: 0.028\n",
      "[3,  1800] loss: 0.035\n",
      "[4,   100] loss: 0.035\n",
      "[4,   200] loss: 0.022\n",
      "[4,   300] loss: 0.031\n",
      "[4,   400] loss: 0.030\n",
      "[4,   500] loss: 0.029\n",
      "[4,   600] loss: 0.036\n",
      "[4,   700] loss: 0.029\n",
      "[4,   800] loss: 0.024\n",
      "[4,   900] loss: 0.035\n",
      "[4,  1000] loss: 0.027\n",
      "[4,  1100] loss: 0.036\n",
      "[4,  1200] loss: 0.032\n",
      "[4,  1300] loss: 0.028\n",
      "[4,  1400] loss: 0.028\n",
      "[4,  1500] loss: 0.031\n",
      "[4,  1600] loss: 0.016\n",
      "[4,  1700] loss: 0.022\n",
      "[4,  1800] loss: 0.041\n",
      "[5,   100] loss: 0.023\n",
      "[5,   200] loss: 0.016\n",
      "[5,   300] loss: 0.017\n",
      "[5,   400] loss: 0.013\n",
      "[5,   500] loss: 0.020\n",
      "[5,   600] loss: 0.024\n",
      "[5,   700] loss: 0.020\n",
      "[5,   800] loss: 0.030\n",
      "[5,   900] loss: 0.031\n",
      "[5,  1000] loss: 0.017\n",
      "[5,  1100] loss: 0.027\n",
      "[5,  1200] loss: 0.030\n",
      "[5,  1300] loss: 0.026\n",
      "[5,  1400] loss: 0.026\n",
      "[5,  1500] loss: 0.023\n",
      "[5,  1600] loss: 0.023\n",
      "[5,  1700] loss: 0.040\n",
      "[5,  1800] loss: 0.021\n",
      "[6,   100] loss: 0.022\n",
      "[6,   200] loss: 0.013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6,   300] loss: 0.012\n",
      "[6,   400] loss: 0.015\n",
      "[6,   500] loss: 0.015\n",
      "[6,   600] loss: 0.027\n",
      "[6,   700] loss: 0.019\n",
      "[6,   800] loss: 0.017\n",
      "[6,   900] loss: 0.026\n",
      "[6,  1000] loss: 0.024\n",
      "[6,  1100] loss: 0.015\n",
      "[6,  1200] loss: 0.019\n",
      "[6,  1300] loss: 0.019\n",
      "[6,  1400] loss: 0.029\n",
      "[6,  1500] loss: 0.020\n",
      "[6,  1600] loss: 0.016\n",
      "[6,  1700] loss: 0.014\n",
      "[6,  1800] loss: 0.016\n",
      "[7,   100] loss: 0.008\n",
      "[7,   200] loss: 0.012\n",
      "[7,   300] loss: 0.017\n",
      "[7,   400] loss: 0.013\n",
      "[7,   500] loss: 0.012\n",
      "[7,   600] loss: 0.023\n",
      "[7,   700] loss: 0.023\n",
      "[7,   800] loss: 0.011\n",
      "[7,   900] loss: 0.014\n",
      "[7,  1000] loss: 0.013\n",
      "[7,  1100] loss: 0.009\n",
      "[7,  1200] loss: 0.016\n",
      "[7,  1300] loss: 0.017\n",
      "[7,  1400] loss: 0.012\n",
      "[7,  1500] loss: 0.018\n",
      "[7,  1600] loss: 0.012\n",
      "[7,  1700] loss: 0.016\n",
      "[7,  1800] loss: 0.030\n",
      "[8,   100] loss: 0.009\n",
      "[8,   200] loss: 0.016\n",
      "[8,   300] loss: 0.008\n",
      "[8,   400] loss: 0.014\n",
      "[8,   500] loss: 0.008\n",
      "[8,   600] loss: 0.012\n",
      "[8,   700] loss: 0.005\n",
      "[8,   800] loss: 0.011\n",
      "[8,   900] loss: 0.008\n",
      "[8,  1000] loss: 0.026\n",
      "[8,  1100] loss: 0.018\n",
      "[8,  1200] loss: 0.017\n",
      "[8,  1300] loss: 0.018\n",
      "[8,  1400] loss: 0.014\n",
      "[8,  1500] loss: 0.013\n",
      "[8,  1600] loss: 0.011\n",
      "[8,  1700] loss: 0.021\n",
      "[8,  1800] loss: 0.014\n",
      "[9,   100] loss: 0.010\n",
      "[9,   200] loss: 0.011\n",
      "[9,   300] loss: 0.021\n",
      "[9,   400] loss: 0.012\n",
      "[9,   500] loss: 0.009\n",
      "[9,   600] loss: 0.010\n",
      "[9,   700] loss: 0.005\n",
      "[9,   800] loss: 0.013\n",
      "[9,   900] loss: 0.006\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "PRINT_EVERY = 100\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Comment this line:\n",
    "#net = ExampleModel()\n",
    "\n",
    "# Uncomment this line\n",
    "net = StudentModel()\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "if type(net) == ExampleModel:\n",
    "  print(\"WARNING! Running example model. Uncomment line to run your own model!\")\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "if USE_CUDA:\n",
    "  net = net.cuda()\n",
    "  print(\"Using GPU\")\n",
    "else:\n",
    "  print(\"Not using GPU\")\n",
    "  \n",
    "net.train()\n",
    "  \n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        if USE_CUDA:\n",
    "          inputs = inputs.cuda()\n",
    "          labels = labels.cuda()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % PRINT_EVERY == PRINT_EVERY - 1:    # print every PRINT_EVERY mini-batches\n",
    "            #show_image(torchvision.utils.make_grid(inputs.data))\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0SLImjx1QE1",
    "colab_type": "text"
   },
   "source": [
    "## Testing\n",
    "\n",
    "Use the below cell to test your model on the MNIST test set. By right you should only run your model on the test set once, before publishing your results. In this assignment, we will allow you to re-use the test set, which is technically an incorrect practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jo1Zkn5EyNG7",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85.0
    },
    "outputId": "7cf29352-f5d6-40ec-e372-209fad1da8ce",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.525469354789E12,
     "user_tz": 240.0,
     "elapsed": 2012.0,
     "user": {
      "displayName": "Disheng Zheng",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "111016125128010800049"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 99.02%\n",
      "Correct: 9902/10000\n",
      "\n",
      "Congratulations! You beat the 99% barrier!\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "net.eval()\n",
    "\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    images = Variable(images, volatile=True)\n",
    "    if USE_CUDA:\n",
    "      images, labels = images.cuda(), labels.cuda()\n",
    "    \n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "acc = 100 * correct / total\n",
    "    \n",
    "print(f'Accuracy of the network on the 10000 test images: {acc}%')\n",
    "print(f'Correct: {correct}/{total}')\n",
    "print(\"\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "if acc >= 99:\n",
    "  \n",
    "  print(\"Congratulations! You beat the 99% barrier!\")\n",
    "else:\n",
    "  print(\"Sorry, but you can do better. Try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmLl5haX4Om_",
    "colab_type": "text"
   },
   "source": [
    "### Per-class accuracy\n",
    "\n",
    "Run the below cell to see which digits your model is better at recognizing and which digits it gets confused by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qp8POK0dyOKn",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 187.0
    },
    "outputId": "959c06f1-9db5-4fa8-e3d3-c183dd65feb5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.525469361186E12,
     "user_tz": 240.0,
     "elapsed": 2589.0,
     "user": {
      "displayName": "Disheng Zheng",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "111016125128010800049"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of     0 : 99 %\n",
      "Accuracy of     1 : 99 %\n",
      "Accuracy of     2 : 100 %\n",
      "Accuracy of     3 : 98 %\n",
      "Accuracy of     4 : 100 %\n",
      "Accuracy of     5 : 97 %\n",
      "Accuracy of     6 : 100 %\n",
      "Accuracy of     7 : 100 %\n",
      "Accuracy of     8 : 100 %\n",
      "Accuracy of     9 : 97 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    images = Variable(images, volatile=True)\n",
    "    if USE_CUDA:\n",
    "      images, labels = images.cuda(), labels.cuda()\n",
    "    \n",
    "    outputs = net(images)\n",
    "    \n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "    for i in range(4):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i]\n",
    "        class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (i, 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CS5670_Project5_MNISTChallenge.ipynb 2",
   "version": "0.3.2",
   "views": {},
   "default_view": {},
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
